Memory Search

Descripción
Memory Search es un agente de investigación profunda en terminal. Un nodo jefe decide si responder con memoria/RAG local o expandir a la web; cuando falta evidencia, descompone la consulta en subtareas enfocadas. Un nodo juntador integra todos los hallazgos en una única respuesta final.

Arquitectura
- Nodo jefe: evalúa si la memoria vectorial resuelve la consulta; si no es suficiente, planifica subtareas y criterios de búsqueda.
- Trabajadores: ejecutan búsquedas web y consultas a la memoria; extraen texto, limpian y seleccionan fragmentos relevantes.
- Nodo juntador: fusiona resultados, reconcilia inconsistencias y emite una respuesta coherente.

Componentes
- main.py: script autocontenido que lee un JSON por stdin con la consulta y credenciales, realiza RAG y/o web scraping y escribe un JSON con el campo answer en stdout. Logs y trazas van a stderr.
- scripts opcionales: utilidades para preparar datos locales (vectorstore Chroma) y pruebas de scraping.

Instalación
- Python 3.10+ y dependencias: langchain_openai, langchain_community, chromadb, bs4, requests. Instala con pip: pip install langchain-openai langchain-community chromadb beautifulsoup4 requests.
- Configuración de clave: exporta tu API key del modelo en el entorno (OPENAI_API_KEY) o pásala en el JSON de entrada.[1]

Uso (terminal)
- Entrada: un JSON por stdin con al menos la consulta y la API key. Ejemplo:
  echo '{"prompt":"explica transformers con un ejemplo","api":"sk-..."}' | python main.py [1]  
- Salida: un único JSON en stdout con el campo answer, por ejemplo:
  {"answer":"...texto final..."}[1]
- Logs: cualquier detalle de RAG/web/LLM aparece en stderr para depuración, sin contaminar la salida.

Diseño
- Separación de roles: planificar, buscar y juntar reduce alucinaciones y mejora cobertura.
- Memoria + web: usa vectorstore local si existe; si no cubre, amplía con web scraping selectivo.
- Proceso único: se ejecuta una vez por consulta; ideal para pipelines y cron jobs.

Limitaciones
- Requiere conectividad para scraping y acceso al modelo.
- La calidad del juntado depende de la diversidad y confiabilidad de las fuentes recuperadas.

Roadmap

- Citas y enlaces estructurados en el output para trazabilidad.
- Servidor HTTP opcional (FastAPI) para baja latencia y concurrencia.
- Incremental indexing y memoria persistente con políticas de recorte.